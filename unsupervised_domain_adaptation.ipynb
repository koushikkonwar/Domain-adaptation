{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervised domain adaptation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_laqnzFvgd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as tvtf\n",
        "import torchvision as tv\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oralPHJmwgWp",
        "colab_type": "code",
        "outputId": "4d3d506b-3515-4bee-c1fa-21b52ac541d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOl4Qfrlw7FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size= 28\n",
        "batch_size = 64\n",
        "\n",
        "tf_source = tvtf.Compose([\n",
        "    tvtf.Resize(image_size),\n",
        "    tvtf.ToTensor(),\n",
        "    tvtf.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "\n",
        "tf_target = tvtf.Compose([\n",
        "    tvtf.Resize(image_size),\n",
        "    tvtf.ToTensor(),\n",
        "    tvtf.Normalize(mean=(0.1307,), std=(0.1307,))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZVPud5nx9k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_target = tv.datasets.MNIST(root='C:/Users/mtech _1/Desktop/gsoc',train=True, transform=tf_target,download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ8eAPSswNSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9CLwrLXyIsf",
        "colab_type": "code",
        "outputId": "52b56a66-93f6-4d6f-d62c-9840eed3719d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ds_source = tv.datasets.SVHN(root='C:/Users/mtech _1/Desktop/gsoc',split='train', transform=tf_source,download=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: C:/Users/mtech _1/Desktop/gsoc/train_32x32.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQkvisn6ysOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
        "dl_target = torch.utils.data.DataLoader(dt_target, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBZ_vIBP0DRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class GradientReversalFn(Function):\n",
        "  @staticmethod\n",
        "  def forward(self, x, alpha):\n",
        "    self.alpha=alpha\n",
        "    \n",
        "    return x.view_as(x)\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(self, grad_output):\n",
        "    output = grad_output.neg()*self.alpha\n",
        "    \n",
        "    return output, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOCxORC7mYOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DACNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.feature_extractor=nn.Sequential(\n",
        "        nn.Conv2d(3,64,kernel_size=5),\n",
        "        nn.BatchNorm2d(64), nn.MaxPool2d(2),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(64,64, kernel_size=5),\n",
        "        nn.BatchNorm2d(64),nn.Dropout2d(),nn.MaxPool2d(2),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(64,128,kernel_size=4),\n",
        "        \n",
        "    )\n",
        "    \n",
        "    self.class_classifier=nn.Sequential(\n",
        "        nn.Linear(128*1*1,3072), nn.BatchNorm1d(3072),nn.Dropout2d(),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(3072,2048), nn.BatchNorm1d(2048),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(2048,10),\n",
        "        nn.LogSoftmax(dim=1),\n",
        "    )\n",
        "    \n",
        "    self.domain_classifier=nn.Sequential(\n",
        "        nn.Linear(128*1*1,1024),nn.BatchNorm1d(1024),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(1024,1024),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(1024,10),\n",
        "        nn.LogSoftmax(dim=1),\n",
        "    )\n",
        "    \n",
        "    \n",
        "    \n",
        "  def forward(self, x, grl_lambda=1.0):\n",
        "    \n",
        "    x= x.expand(x.data.shape[0], 3, image_size, image_size)\n",
        "    \n",
        "    features = self.feature_extractor(x)\n",
        "    features= features.view(-1,128*1*1)\n",
        "    reverse_features= GradientReversalFn.apply(features, grl_lambda)\n",
        "    \n",
        "    class_pred = self.class_classifier(features)\n",
        "    domain_pred = self.domain_classifier(reverse_features)\n",
        "    \n",
        "    return class_pred, domain_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq7j_ljR2yLH",
        "colab_type": "code",
        "outputId": "148e1bde-98c0-448e-9115-9f8de17195f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model = DACNN()\n",
        "\n",
        "x_s, y_s = next(iter(dl_source))\n",
        "x_t, y_t = next(iter(dl_target))\n",
        "\n",
        "print('source domain: ', x_s.shape, y_s.shape)\n",
        "print('target domain: ', x_t.shape, y_t.shape)\n",
        "\n",
        "model(x_s)\n",
        "model(x_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source domain:  torch.Size([64, 3, 28, 28]) torch.Size([64])\n",
            "target domain:  torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-2.2455, -2.4973, -2.1269, -2.5838, -2.1413, -2.3132, -2.0836, -2.4933,\n",
              "          -2.4097, -2.2660],\n",
              "         [-2.3298, -2.4438, -2.1230, -2.1562, -2.5305, -2.3406, -2.1839, -2.2907,\n",
              "          -2.4515, -2.2579],\n",
              "         [-2.3060, -2.3156, -2.1187, -2.3726, -2.3722, -2.5651, -2.2848, -2.3332,\n",
              "          -2.5099, -1.9806],\n",
              "         [-2.3211, -2.4496, -2.2610, -2.3752, -2.5351, -2.3456, -2.1679, -2.0964,\n",
              "          -2.5222, -2.0744],\n",
              "         [-2.1515, -2.6835, -2.2961, -2.4822, -2.4471, -2.5603, -2.4422, -2.2533,\n",
              "          -1.8362, -2.1550],\n",
              "         [-2.1097, -2.2974, -2.5170, -2.2955, -2.4851, -2.1845, -2.3403, -2.1935,\n",
              "          -2.2466, -2.4376],\n",
              "         [-2.2209, -2.3716, -2.3230, -2.2865, -2.2252, -2.7136, -1.9814, -2.5168,\n",
              "          -2.4242, -2.1449],\n",
              "         [-2.3101, -2.7095, -2.5525, -2.6182, -2.3795, -2.1501, -2.2054, -1.5271,\n",
              "          -2.6245, -2.5976],\n",
              "         [-2.3390, -2.2948, -2.2797, -2.3904, -2.4056, -2.1884, -2.1112, -2.2347,\n",
              "          -2.5538, -2.2959],\n",
              "         [-2.5101, -2.2029, -2.3163, -2.0918, -2.6570, -2.1514, -2.0263, -2.6304,\n",
              "          -2.2174, -2.4428],\n",
              "         [-2.2816, -2.3480, -3.0106, -2.0073, -2.4903, -2.2375, -1.9629, -2.3299,\n",
              "          -2.2806, -2.4179],\n",
              "         [-2.1796, -2.3938, -2.4097, -2.2811, -2.2116, -2.2428, -2.2115, -2.3956,\n",
              "          -2.3934, -2.3438],\n",
              "         [-2.3231, -2.4059, -2.7228, -2.3525, -2.3162, -2.1481, -1.8437, -2.1325,\n",
              "          -2.6204, -2.4519],\n",
              "         [-2.0705, -2.1784, -2.4149, -2.4359, -2.3583, -2.3225, -2.2831, -2.2550,\n",
              "          -2.5056, -2.2762],\n",
              "         [-2.3747, -2.5563, -2.3777, -2.1415, -2.1509, -2.3144, -1.9927, -2.3899,\n",
              "          -2.5147, -2.3531],\n",
              "         [-1.9153, -2.4331, -2.1271, -2.6247, -2.2257, -2.6633, -2.2616, -2.0319,\n",
              "          -2.5792, -2.4713],\n",
              "         [-2.3214, -2.4978, -2.3252, -2.2756, -2.5171, -2.0960, -2.2215, -2.4523,\n",
              "          -2.5486, -1.9469],\n",
              "         [-2.3163, -2.3561, -2.2538, -2.3340, -2.4101, -2.2872, -2.1120, -2.1779,\n",
              "          -2.7122, -2.1834],\n",
              "         [-2.4253, -2.5197, -2.4266, -2.3899, -2.2057, -2.3482, -1.9770, -2.2788,\n",
              "          -2.3978, -2.1753],\n",
              "         [-2.2774, -2.3836, -2.2813, -2.4977, -2.3855, -2.1191, -2.1087, -2.1882,\n",
              "          -2.6190, -2.2806],\n",
              "         [-2.5864, -2.3149, -2.2570, -2.2547, -2.1456, -2.7289, -2.1290, -1.9182,\n",
              "          -2.6404, -2.3340],\n",
              "         [-2.4761, -2.7439, -2.4946, -2.4983, -2.8970, -2.2237, -1.7385, -2.1285,\n",
              "          -2.2891, -2.0612],\n",
              "         [-2.3277, -2.5032, -2.2360, -2.0949, -2.4699, -2.4905, -1.9975, -2.0759,\n",
              "          -2.5173, -2.5079],\n",
              "         [-2.3223, -2.1801, -2.3489, -2.1514, -2.2893, -2.4843, -2.3263, -2.1079,\n",
              "          -2.4855, -2.4081],\n",
              "         [-2.2658, -2.5012, -2.4286, -2.6531, -2.4022, -2.6076, -1.9656, -2.0884,\n",
              "          -1.9858, -2.4055],\n",
              "         [-2.0743, -2.5850, -2.4200, -2.3995, -2.4973, -2.1993, -2.0977, -2.2389,\n",
              "          -2.3116, -2.3262],\n",
              "         [-2.3889, -2.6267, -2.2274, -2.3085, -2.3979, -2.5347, -2.2344, -2.1121,\n",
              "          -2.4978, -1.9091],\n",
              "         [-2.3706, -2.4159, -2.3932, -2.4844, -2.1182, -2.1231, -2.1031, -2.2560,\n",
              "          -2.3700, -2.4970],\n",
              "         [-2.4065, -2.9888, -1.9948, -2.1563, -2.2701, -2.3368, -2.1645, -2.2000,\n",
              "          -2.5322, -2.2760],\n",
              "         [-2.2004, -2.0849, -2.2183, -2.4082, -2.3750, -2.4352, -2.2748, -2.2129,\n",
              "          -2.3989, -2.4946],\n",
              "         [-2.2097, -2.5767, -2.0966, -2.6254, -2.2100, -2.5125, -2.2748, -2.0847,\n",
              "          -2.3024, -2.2924],\n",
              "         [-2.2310, -2.4245, -2.0735, -2.2919, -2.3851, -1.9022, -2.6238, -2.3556,\n",
              "          -2.5226, -2.4270],\n",
              "         [-2.2736, -2.2660, -2.1857, -2.5857, -2.4894, -2.5829, -2.3759, -1.9109,\n",
              "          -2.6441, -2.0015],\n",
              "         [-2.4506, -2.5781, -2.3387, -2.0624, -2.5018, -2.5313, -1.8856, -2.2851,\n",
              "          -2.3762, -2.2412],\n",
              "         [-2.4442, -2.6827, -2.4507, -2.3888, -2.1790, -2.4191, -2.1365, -2.2257,\n",
              "          -2.3782, -1.9247],\n",
              "         [-2.2070, -2.3054, -2.0413, -2.4024, -2.5552, -2.1461, -2.2971, -2.1402,\n",
              "          -2.4916, -2.5995],\n",
              "         [-2.1570, -2.2884, -2.2582, -2.5478, -2.4362, -2.3903, -1.8749, -2.3190,\n",
              "          -2.5916, -2.3604],\n",
              "         [-2.6387, -2.4601, -1.9558, -2.4821, -2.3280, -2.3035, -2.3143, -2.2632,\n",
              "          -2.4001, -2.0625],\n",
              "         [-2.2114, -2.1878, -2.4553, -2.2500, -2.2969, -2.3314, -2.1753, -2.4282,\n",
              "          -2.3670, -2.3665],\n",
              "         [-2.0816, -2.3341, -2.2125, -2.6974, -2.6355, -2.1368, -2.0028, -2.0595,\n",
              "          -2.8340, -2.4013],\n",
              "         [-2.3064, -2.2948, -2.4208, -2.5476, -2.3528, -2.2402, -2.1120, -2.2582,\n",
              "          -2.3256, -2.2286],\n",
              "         [-2.3636, -2.8708, -2.0460, -2.1318, -2.6214, -2.0073, -2.3344, -2.1024,\n",
              "          -2.3109, -2.5753],\n",
              "         [-2.2353, -2.5227, -2.1346, -2.4159, -2.4879, -2.4269, -2.2667, -1.9576,\n",
              "          -2.3798, -2.3391],\n",
              "         [-2.3139, -2.6669, -2.2567, -2.5862, -2.1722, -2.2811, -2.0848, -2.4470,\n",
              "          -2.2462, -2.1297],\n",
              "         [-2.5366, -2.6365, -2.2417, -2.4498, -2.1655, -1.7688, -2.0509, -2.6044,\n",
              "          -2.5099, -2.4351],\n",
              "         [-1.8367, -2.5148, -2.2835, -2.4981, -2.3229, -2.3670, -2.1883, -2.4609,\n",
              "          -2.4606, -2.2925],\n",
              "         [-2.5541, -2.4846, -2.1800, -2.1719, -2.2226, -2.3955, -2.2792, -2.0670,\n",
              "          -2.5025, -2.2866],\n",
              "         [-2.1304, -2.4891, -2.3655, -2.4281, -2.5339, -2.4256, -2.0065, -2.2797,\n",
              "          -2.1362, -2.3701],\n",
              "         [-2.3945, -2.4854, -2.1270, -2.1259, -2.3619, -2.3219, -1.9592, -2.1456,\n",
              "          -2.7792, -2.5957],\n",
              "         [-2.2076, -2.4241, -2.2010, -2.5022, -2.2055, -2.0584, -2.1845, -2.7441,\n",
              "          -2.3103, -2.3543],\n",
              "         [-2.0424, -2.0164, -2.3059, -2.2626, -2.5321, -2.3276, -2.2776, -2.4859,\n",
              "          -2.4787, -2.4447],\n",
              "         [-2.2959, -2.7995, -2.4857, -2.2331, -2.2927, -1.9767, -2.1856, -2.0491,\n",
              "          -2.7395, -2.2766],\n",
              "         [-2.2257, -2.6411, -2.5278, -2.2911, -2.2160, -2.5070, -2.0024, -2.1027,\n",
              "          -2.5042, -2.2045],\n",
              "         [-2.0734, -2.6675, -2.3441, -2.2705, -2.1651, -2.2270, -2.4304, -2.4347,\n",
              "          -2.7045, -1.9625],\n",
              "         [-2.3648, -2.7141, -2.1215, -2.4775, -1.9944, -2.4687, -2.3277, -2.4142,\n",
              "          -2.5656, -1.8882],\n",
              "         [-2.3763, -2.3927, -2.0371, -2.2172, -2.4106, -2.4224, -2.1472, -2.2089,\n",
              "          -2.3334, -2.5994],\n",
              "         [-1.9956, -2.2222, -2.1847, -2.4936, -2.6373, -2.2202, -2.4235, -2.2048,\n",
              "          -2.3200, -2.4857],\n",
              "         [-2.4716, -2.2914, -2.1590, -2.2949, -2.2486, -2.4811, -2.3552, -2.5416,\n",
              "          -2.3918, -1.9385],\n",
              "         [-2.0400, -2.4806, -2.3084, -2.1022, -2.4001, -2.3166, -2.2754, -2.2694,\n",
              "          -2.5052, -2.4355],\n",
              "         [-2.5849, -2.3201, -2.0670, -2.3318, -2.3541, -2.3872, -2.1495, -2.3120,\n",
              "          -2.5614, -2.0957],\n",
              "         [-2.5986, -2.8817, -2.3517, -2.0050, -2.3028, -2.2468, -2.0753, -2.5312,\n",
              "          -2.0799, -2.2637],\n",
              "         [-2.4112, -2.4734, -2.5330, -2.2528, -2.2757, -2.5291, -1.9833, -2.0186,\n",
              "          -2.5048, -2.2361],\n",
              "         [-2.2764, -2.5513, -2.3516, -2.2246, -2.4769, -2.0830, -2.1925, -2.4310,\n",
              "          -2.2513, -2.2767],\n",
              "         [-2.4450, -2.6168, -2.0446, -2.4977, -2.4455, -2.3556, -2.0926, -2.0798,\n",
              "          -2.8624, -1.9550]], grad_fn=<LogSoftmaxBackward>),\n",
              " tensor([[-2.4143, -2.3147, -2.3414, -2.1661, -2.3254, -2.2269, -2.3941, -2.1949,\n",
              "          -2.3992, -2.2835],\n",
              "         [-2.2832, -2.4382, -2.1803, -2.2098, -2.3874, -2.3038, -2.3973, -2.1520,\n",
              "          -2.3822, -2.3362],\n",
              "         [-2.4096, -2.2846, -2.2717, -2.2582, -2.2712, -2.2597, -2.3433, -2.2770,\n",
              "          -2.3158, -2.3455],\n",
              "         [-2.3747, -2.4165, -2.2922, -2.1629, -2.3614, -2.2422, -2.3661, -2.2663,\n",
              "          -2.4171, -2.1675],\n",
              "         [-2.3679, -2.4194, -2.1542, -2.2538, -2.3453, -2.2449, -2.4440, -2.1653,\n",
              "          -2.3608, -2.3153],\n",
              "         [-2.3997, -2.5277, -2.1877, -2.1498, -2.4508, -2.1801, -2.2678, -2.2969,\n",
              "          -2.3261, -2.3058],\n",
              "         [-2.3444, -2.5348, -2.2881, -2.2527, -2.2720, -2.2662, -2.4045, -2.1574,\n",
              "          -2.3482, -2.2076],\n",
              "         [-2.3186, -2.5131, -2.2488, -2.1309, -2.3201, -2.2667, -2.3300, -2.2985,\n",
              "          -2.4321, -2.2178],\n",
              "         [-2.4269, -2.5253, -2.1442, -2.2348, -2.3312, -2.2787, -2.4372, -2.2289,\n",
              "          -2.3170, -2.1692],\n",
              "         [-2.2691, -2.4936, -2.2733, -2.2200, -2.2816, -2.3790, -2.2683, -2.2447,\n",
              "          -2.3467, -2.2777],\n",
              "         [-2.3325, -2.5801, -2.1441, -2.2026, -2.3929, -2.3029, -2.3755, -2.2162,\n",
              "          -2.3992, -2.1595],\n",
              "         [-2.3654, -2.4124, -2.2931, -2.2631, -2.3161, -2.1857, -2.4606, -2.2735,\n",
              "          -2.2244, -2.2635],\n",
              "         [-2.3408, -2.5435, -2.1287, -2.1725, -2.4162, -2.2474, -2.3880, -2.2009,\n",
              "          -2.3956, -2.2661],\n",
              "         [-2.3199, -2.4234, -2.3278, -2.2294, -2.3817, -2.3385, -2.4307, -2.1600,\n",
              "          -2.1874, -2.2670],\n",
              "         [-2.3203, -2.4159, -2.2838, -2.2887, -2.4822, -2.2326, -2.3263, -2.1205,\n",
              "          -2.2741, -2.3243],\n",
              "         [-2.3980, -2.4461, -2.1336, -2.2336, -2.2725, -2.2476, -2.3696, -2.2857,\n",
              "          -2.3939, -2.2856],\n",
              "         [-2.3641, -2.4915, -2.1515, -2.0214, -2.3536, -2.3553, -2.5198, -2.1950,\n",
              "          -2.4339, -2.2540],\n",
              "         [-2.2876, -2.4787, -2.1159, -2.1249, -2.4663, -2.2227, -2.4362, -2.2442,\n",
              "          -2.4821, -2.2598],\n",
              "         [-2.3137, -2.4555, -2.1781, -2.2644, -2.3390, -2.2440, -2.3871, -2.2960,\n",
              "          -2.3296, -2.2463],\n",
              "         [-2.3812, -2.4615, -2.2634, -2.2042, -2.3285, -2.2850, -2.2847, -2.1913,\n",
              "          -2.4004, -2.2592],\n",
              "         [-2.3255, -2.3419, -2.2549, -2.1893, -2.3314, -2.2331, -2.3374, -2.2660,\n",
              "          -2.4289, -2.3386],\n",
              "         [-2.2962, -2.4740, -2.1834, -2.1514, -2.3159, -2.2957, -2.3202, -2.2703,\n",
              "          -2.4504, -2.3130],\n",
              "         [-2.3568, -2.5934, -2.1555, -2.1776, -2.4114, -2.1354, -2.4140, -2.2096,\n",
              "          -2.3988, -2.2688],\n",
              "         [-2.3212, -2.4258, -2.2326, -2.2551, -2.3667, -2.3344, -2.3933, -2.2803,\n",
              "          -2.2862, -2.1590],\n",
              "         [-2.2363, -2.4729, -2.0992, -2.1286, -2.3484, -2.2914, -2.4265, -2.3410,\n",
              "          -2.4318, -2.3212],\n",
              "         [-2.4485, -2.4905, -2.1784, -2.2473, -2.3057, -2.3035, -2.3811, -2.2272,\n",
              "          -2.3228, -2.1723],\n",
              "         [-2.3396, -2.4677, -2.3281, -2.1285, -2.4456, -2.2590, -2.3086, -2.1995,\n",
              "          -2.4056, -2.2004],\n",
              "         [-2.3285, -2.4627, -2.1158, -2.2823, -2.4711, -2.1637, -2.2851, -2.3063,\n",
              "          -2.4544, -2.2250],\n",
              "         [-2.3269, -2.5750, -2.0295, -2.1182, -2.3969, -2.2958, -2.3056, -2.3532,\n",
              "          -2.5310, -2.2203],\n",
              "         [-2.3601, -2.3979, -2.2081, -2.2393, -2.3804, -2.3001, -2.1994, -2.3026,\n",
              "          -2.3480, -2.3121],\n",
              "         [-2.3084, -2.4600, -2.1446, -2.2371, -2.2629, -2.3348, -2.3202, -2.2486,\n",
              "          -2.4762, -2.2776],\n",
              "         [-2.4237, -2.5870, -2.1783, -2.1689, -2.3751, -2.3501, -2.3532, -2.2521,\n",
              "          -2.2860, -2.1341],\n",
              "         [-2.3355, -2.3723, -2.2141, -2.2470, -2.3425, -2.3261, -2.3793, -2.2072,\n",
              "          -2.3522, -2.2686],\n",
              "         [-2.2977, -2.5810, -2.2223, -2.1058, -2.3707, -2.3101, -2.3720, -2.2449,\n",
              "          -2.3390, -2.2500],\n",
              "         [-2.4543, -2.5209, -2.0880, -2.2373, -2.3430, -2.3487, -2.2889, -2.4032,\n",
              "          -2.3144, -2.1133],\n",
              "         [-2.4601, -2.4098, -2.1766, -2.2112, -2.4096, -2.2455, -2.4080, -2.2324,\n",
              "          -2.3413, -2.1844],\n",
              "         [-2.3904, -2.4948, -2.1386, -2.1777, -2.3105, -2.3163, -2.3289, -2.1555,\n",
              "          -2.4432, -2.3353],\n",
              "         [-2.3580, -2.4720, -2.2581, -2.2293, -2.3663, -2.3012, -2.3448, -2.4559,\n",
              "          -2.2266, -2.0771],\n",
              "         [-2.3335, -2.4357, -2.0950, -2.2047, -2.4587, -2.3143, -2.4726, -2.2467,\n",
              "          -2.3655, -2.1737],\n",
              "         [-2.3649, -2.4836, -2.1872, -2.3117, -2.3139, -2.2382, -2.3773, -2.1643,\n",
              "          -2.4427, -2.1964],\n",
              "         [-2.3091, -2.4394, -2.2232, -2.1955, -2.3157, -2.3067, -2.4206, -2.2626,\n",
              "          -2.4335, -2.1637],\n",
              "         [-2.2607, -2.4285, -2.1825, -2.1773, -2.4345, -2.3480, -2.4260, -2.2377,\n",
              "          -2.3853, -2.1974],\n",
              "         [-2.2996, -2.5141, -2.1838, -2.2833, -2.3617, -2.3937, -2.1972, -2.1915,\n",
              "          -2.4745, -2.1931],\n",
              "         [-2.4071, -2.4214, -2.1903, -2.1952, -2.3892, -2.3112, -2.3588, -2.1988,\n",
              "          -2.3591, -2.2342],\n",
              "         [-2.3946, -2.4658, -2.1714, -2.2918, -2.3188, -2.2991, -2.2587, -2.3226,\n",
              "          -2.3687, -2.1724],\n",
              "         [-2.4084, -2.4946, -2.0657, -2.2482, -2.4287, -2.3122, -2.2349, -2.2186,\n",
              "          -2.4901, -2.2133],\n",
              "         [-2.3307, -2.5142, -2.2316, -2.2568, -2.3865, -2.2237, -2.3946, -2.1560,\n",
              "          -2.3079, -2.2712],\n",
              "         [-2.3616, -2.3826, -2.1689, -2.3537, -2.3214, -2.3433, -2.2808, -2.2478,\n",
              "          -2.3907, -2.2023],\n",
              "         [-2.3193, -2.4557, -2.2557, -2.1680, -2.3427, -2.2905, -2.3948, -2.2641,\n",
              "          -2.3400, -2.2263],\n",
              "         [-2.3657, -2.4526, -2.0966, -2.2604, -2.4013, -2.3286, -2.2864, -2.1257,\n",
              "          -2.4690, -2.3108],\n",
              "         [-2.4686, -2.4733, -2.1015, -2.2753, -2.3295, -2.3429, -2.2950, -2.3495,\n",
              "          -2.3119, -2.1431],\n",
              "         [-2.2828, -2.5673, -2.2273, -2.0915, -2.3851, -2.3500, -2.3216, -2.2513,\n",
              "          -2.4046, -2.2182],\n",
              "         [-2.4056, -2.5466, -2.1481, -2.1956, -2.3308, -2.2522, -2.3376, -2.1868,\n",
              "          -2.3914, -2.2954],\n",
              "         [-2.2691, -2.4501, -2.3368, -2.1765, -2.3101, -2.2651, -2.4342, -2.1718,\n",
              "          -2.3640, -2.2879],\n",
              "         [-2.3011, -2.4121, -2.1060, -2.2844, -2.3780, -2.3072, -2.3495, -2.1844,\n",
              "          -2.4272, -2.3215],\n",
              "         [-2.1886, -2.4932, -2.1209, -2.0165, -2.5749, -2.2777, -2.5146, -2.2679,\n",
              "          -2.3657, -2.3512],\n",
              "         [-2.2990, -2.3635, -2.2712, -2.0476, -2.4277, -2.3421, -2.4359, -2.2213,\n",
              "          -2.3079, -2.3699],\n",
              "         [-2.2777, -2.4006, -2.2275, -2.1801, -2.3635, -2.2663, -2.3322, -2.2643,\n",
              "          -2.4338, -2.3076],\n",
              "         [-2.3238, -2.5834, -2.1328, -2.2248, -2.3377, -2.3082, -2.3406, -2.2389,\n",
              "          -2.2873, -2.3064],\n",
              "         [-2.2951, -2.4161, -2.2934, -2.1518, -2.4384, -2.2392, -2.3063, -2.2294,\n",
              "          -2.4240, -2.2716],\n",
              "         [-2.3344, -2.5240, -2.1929, -2.1723, -2.2814, -2.2078, -2.4478, -2.1471,\n",
              "          -2.4335, -2.3605],\n",
              "         [-2.2628, -2.3564, -2.2287, -2.2021, -2.4232, -2.2831, -2.4886, -2.2306,\n",
              "          -2.3474, -2.2421],\n",
              "         [-2.3015, -2.4947, -2.2195, -2.1766, -2.3881, -2.4356, -2.1872, -2.2870,\n",
              "          -2.2770, -2.3071],\n",
              "         [-2.3470, -2.4038, -2.3155, -2.3153, -2.2099, -2.2922, -2.3521, -2.2159,\n",
              "          -2.2524, -2.3392]], grad_fn=<LogSoftmaxBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Fsvp6N3ikK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "lr = 1e-3\n",
        "n_epochs=400\n",
        "\n",
        "model=DACNN()\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.08, momentum=0.9)\n",
        "\n",
        "loss_fn_class = torch.nn.NLLLoss()\n",
        "loss_fn_domain= torch.nn.NLLLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgxlWS044lM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size= 2000\n",
        "dl_source= torch.utils.data.DataLoader(ds_source, batch_size)\n",
        "dl_target= torch.utils.data.DataLoader(dt_target, batch_size)\n",
        "\n",
        "max_batches= min(len(dl_source), len(dl_target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDGx7_jXKVCJ",
        "colab_type": "code",
        "outputId": "de13004a-0695-4f4c-cdd9-cc2d8a6a1208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(ds_source), len(dt_target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73257, 60000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt60pjCAmYR2",
        "colab_type": "code",
        "outputId": "5bc67861-54f6-417d-f4a4-8a4b22c7c5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XveguPbivpR",
        "colab_type": "code",
        "outputId": "3b6c7e0b-0d3f-46b8-c78d-4ae76cfd6e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dl_source), len(dl_target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pEEGLVn7kE7",
        "colab_type": "code",
        "outputId": "02631d7a-172f-4ddb-b8f6-cbf5b03507c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "steps=0\n",
        "import numpy as np\n",
        "for epoch_idx in range(n_epochs):\n",
        "  print(f'Epoch{epoch_idx+1:04d}/ {n_epochs:04d}', end= '\\n=============\\n')\n",
        "  dl_source_iter= iter(dl_source)\n",
        "  dl_target_iter= iter(dl_target)\n",
        "  \n",
        "  for batch_idx in range(max_batches):\n",
        "    steps+=1\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    p= float(batch_idx + epoch_idx*max_batches)/(n_epochs*max_batches)\n",
        "    grl_lambda= 2./ (1.+np.exp(-10*p))-1\n",
        "    #grl_lambda= 1\n",
        "    \n",
        "    \n",
        "    x_s,y_s = next(dl_source_iter)\n",
        "    x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "    y_s_domain= torch.zeros(batch_size, dtype= torch.long)\n",
        "    y_s_domain= y_s_domain.to(device)\n",
        "    \n",
        "    class_pred, domain_pred = model(x_s, grl_lambda)\n",
        "    loss_s_label=loss_fn_class(class_pred, y_s)\n",
        "    loss_s_domain = loss_fn_domain(domain_pred, y_s_domain)\n",
        "    \n",
        "    x_t, _ = next(dl_target_iter)\n",
        "    y_t_domain = torch.ones(batch_size, dtype= torch.long)\n",
        "    x_t, y_t_domain = x_t.to(device), y_t_domain.to(device)\n",
        "    \n",
        "    _, domain_pred= model(x_t, grl_lambda)\n",
        "    \n",
        "    loss_t_domain= loss_fn_domain(domain_pred, y_t_domain)\n",
        "    \n",
        "    \n",
        "    loss= loss_t_domain +loss_s_domain + loss_s_label\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    #if (steps%10) == 0:\n",
        "  model.eval()\n",
        "  test_loss1=0\n",
        "  accuracy1=0\n",
        "        \n",
        "  test_loss2=0\n",
        "  accuracy2=0\n",
        "        \n",
        "  for t_images, t_labels in iter(dl_target):\n",
        "    t_images, t_labels = t_images.to(device), t_labels.to(device)\n",
        "    t_logps,_= model(t_images, grl_lambda)\n",
        "    t_loss= loss_fn_class(t_logps, t_labels)\n",
        "    test_loss1 += loss.item()\n",
        "          \n",
        "          \n",
        "    t_ps = torch.exp(t_logps)\n",
        "          \n",
        "    t_top_ps, t_top_class = t_ps.topk(1 ,dim=1)\n",
        "    equality= t_top_class==t_labels.view(*t_top_class.shape)\n",
        "    accuracy1 += torch.mean(equality.type(torch.FloatTensor)).item()\n",
        "          \n",
        "  for s_images, s_labels in iter(dl_source):\n",
        "    s_images, s_labels = s_images.to(device), s_labels.to(device)\n",
        "    s_logps,_= model(s_images, grl_lambda)\n",
        "    s_loss= loss_fn_class(s_logps, s_labels)\n",
        "    test_loss2 += loss.item()\n",
        "          \n",
        "          \n",
        "    s_ps = torch.exp(s_logps)\n",
        "          \n",
        "    s_top_ps, s_top_class = s_ps.topk(1 ,dim=1)\n",
        "    equality= s_top_class==s_labels.view(*s_top_class.shape)\n",
        "    accuracy2 += torch.mean(equality.type(torch.FloatTensor)).item()\n",
        "          \n",
        "        \n",
        "    \n",
        "    \n",
        "  print(f'[{batch_idx+1}/{max_batches}]'\n",
        "        f'class_loss: {loss_s_label.item(): .4f}       '     f's_domain_loss: {loss_s_domain.item():.4f}   '\n",
        "        f't_domain_loss:{loss_t_domain.item():.4f}       '     f'grl_lambda: {grl_lambda:.3f}    '\n",
        "        f'Target accuracy: {accuracy1/len(dl_target):.3f}   '    f'source accuracy: {accuracy2/len(dl_source):.3f}'\n",
        "        )\n",
        "      \n",
        "  model.train()\n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "    #if batch_idx==2:\n",
        "      #print('This is just a demo, stopping.....')\n",
        "      #break "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch0001/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.8885       s_domain_loss: 0.7965   t_domain_loss:0.4775       grl_lambda: 0.012    Target accuracy: 0.534   source accuracy: 0.416\n",
            "Epoch0002/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.5801       s_domain_loss: 0.1747   t_domain_loss:0.0661       grl_lambda: 0.025    Target accuracy: 0.526   source accuracy: 0.226\n",
            "Epoch0003/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.5131       s_domain_loss: 0.3824   t_domain_loss:0.7424       grl_lambda: 0.037    Target accuracy: 0.552   source accuracy: 0.226\n",
            "Epoch0004/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4795       s_domain_loss: 0.1931   t_domain_loss:0.3311       grl_lambda: 0.050    Target accuracy: 0.535   source accuracy: 0.242\n",
            "Epoch0005/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4730       s_domain_loss: 0.6028   t_domain_loss:0.4042       grl_lambda: 0.062    Target accuracy: 0.507   source accuracy: 0.281\n",
            "Epoch0006/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4665       s_domain_loss: 0.2895   t_domain_loss:0.6016       grl_lambda: 0.074    Target accuracy: 0.535   source accuracy: 0.268\n",
            "Epoch0007/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4336       s_domain_loss: 0.4209   t_domain_loss:0.5553       grl_lambda: 0.087    Target accuracy: 0.558   source accuracy: 0.284\n",
            "Epoch0008/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4390       s_domain_loss: 0.3874   t_domain_loss:0.7198       grl_lambda: 0.099    Target accuracy: 0.539   source accuracy: 0.254\n",
            "Epoch0009/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4246       s_domain_loss: 0.7726   t_domain_loss:0.4827       grl_lambda: 0.112    Target accuracy: 0.560   source accuracy: 0.266\n",
            "Epoch0010/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4080       s_domain_loss: 0.5064   t_domain_loss:0.6797       grl_lambda: 0.124    Target accuracy: 0.522   source accuracy: 0.285\n",
            "Epoch0011/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4097       s_domain_loss: 0.5635   t_domain_loss:0.4688       grl_lambda: 0.136    Target accuracy: 0.595   source accuracy: 0.251\n",
            "Epoch0012/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3934       s_domain_loss: 0.4571   t_domain_loss:0.8125       grl_lambda: 0.148    Target accuracy: 0.554   source accuracy: 0.299\n",
            "Epoch0013/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4076       s_domain_loss: 0.6605   t_domain_loss:0.3719       grl_lambda: 0.161    Target accuracy: 0.591   source accuracy: 0.265\n",
            "Epoch0014/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3737       s_domain_loss: 0.3897   t_domain_loss:0.7730       grl_lambda: 0.173    Target accuracy: 0.579   source accuracy: 0.340\n",
            "Epoch0015/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4155       s_domain_loss: 0.4808   t_domain_loss:0.3207       grl_lambda: 0.185    Target accuracy: 0.571   source accuracy: 0.287\n",
            "Epoch0016/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3802       s_domain_loss: 0.4819   t_domain_loss:0.4203       grl_lambda: 0.197    Target accuracy: 0.566   source accuracy: 0.354\n",
            "Epoch0017/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4039       s_domain_loss: 0.5287   t_domain_loss:0.6417       grl_lambda: 0.209    Target accuracy: 0.571   source accuracy: 0.272\n",
            "Epoch0018/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4003       s_domain_loss: 0.7113   t_domain_loss:0.6350       grl_lambda: 0.221    Target accuracy: 0.580   source accuracy: 0.284\n",
            "Epoch0019/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3718       s_domain_loss: 0.7040   t_domain_loss:0.6424       grl_lambda: 0.233    Target accuracy: 0.589   source accuracy: 0.290\n",
            "Epoch0020/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3848       s_domain_loss: 0.7212   t_domain_loss:0.7742       grl_lambda: 0.245    Target accuracy: 0.587   source accuracy: 0.402\n",
            "Epoch0021/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4156       s_domain_loss: 0.9501   t_domain_loss:0.4968       grl_lambda: 0.256    Target accuracy: 0.565   source accuracy: 0.217\n",
            "Epoch0022/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4220       s_domain_loss: 0.7704   t_domain_loss:0.7917       grl_lambda: 0.268    Target accuracy: 0.579   source accuracy: 0.283\n",
            "Epoch0023/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.4580       s_domain_loss: 0.6949   t_domain_loss:1.2468       grl_lambda: 0.279    Target accuracy: 0.576   source accuracy: 0.260\n",
            "Epoch0024/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3470       s_domain_loss: 0.7736   t_domain_loss:0.5448       grl_lambda: 0.291    Target accuracy: 0.588   source accuracy: 0.281\n",
            "Epoch0025/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3435       s_domain_loss: 0.5674   t_domain_loss:0.5891       grl_lambda: 0.302    Target accuracy: 0.629   source accuracy: 0.296\n",
            "Epoch0026/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3792       s_domain_loss: 0.6519   t_domain_loss:0.5586       grl_lambda: 0.314    Target accuracy: 0.568   source accuracy: 0.234\n",
            "Epoch0027/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3487       s_domain_loss: 0.6635   t_domain_loss:0.6037       grl_lambda: 0.325    Target accuracy: 0.624   source accuracy: 0.255\n",
            "Epoch0028/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3680       s_domain_loss: 0.7417   t_domain_loss:0.5643       grl_lambda: 0.336    Target accuracy: 0.581   source accuracy: 0.275\n",
            "Epoch0029/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3465       s_domain_loss: 0.6586   t_domain_loss:0.6898       grl_lambda: 0.347    Target accuracy: 0.619   source accuracy: 0.303\n",
            "Epoch0030/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3474       s_domain_loss: 0.7135   t_domain_loss:0.6226       grl_lambda: 0.358    Target accuracy: 0.629   source accuracy: 0.309\n",
            "Epoch0031/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3132       s_domain_loss: 0.7147   t_domain_loss:0.6587       grl_lambda: 0.369    Target accuracy: 0.624   source accuracy: 0.343\n",
            "Epoch0032/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3235       s_domain_loss: 0.6899   t_domain_loss:0.6548       grl_lambda: 0.380    Target accuracy: 0.633   source accuracy: 0.277\n",
            "Epoch0033/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3107       s_domain_loss: 0.7131   t_domain_loss:0.5587       grl_lambda: 0.390    Target accuracy: 0.620   source accuracy: 0.421\n",
            "Epoch0034/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3406       s_domain_loss: 0.7062   t_domain_loss:0.6003       grl_lambda: 0.401    Target accuracy: 0.633   source accuracy: 0.315\n",
            "Epoch0035/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3185       s_domain_loss: 0.6233   t_domain_loss:0.5841       grl_lambda: 0.411    Target accuracy: 0.623   source accuracy: 0.251\n",
            "Epoch0036/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2986       s_domain_loss: 0.6463   t_domain_loss:0.6486       grl_lambda: 0.422    Target accuracy: 0.643   source accuracy: 0.323\n",
            "Epoch0037/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3171       s_domain_loss: 0.7294   t_domain_loss:0.6501       grl_lambda: 0.432    Target accuracy: 0.633   source accuracy: 0.267\n",
            "Epoch0038/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3259       s_domain_loss: 0.6555   t_domain_loss:0.7057       grl_lambda: 0.442    Target accuracy: 0.639   source accuracy: 0.317\n",
            "Epoch0039/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3148       s_domain_loss: 0.6296   t_domain_loss:0.7677       grl_lambda: 0.452    Target accuracy: 0.657   source accuracy: 0.294\n",
            "Epoch0040/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3376       s_domain_loss: 0.6995   t_domain_loss:0.6331       grl_lambda: 0.462    Target accuracy: 0.605   source accuracy: 0.368\n",
            "Epoch0041/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2999       s_domain_loss: 0.6967   t_domain_loss:0.6324       grl_lambda: 0.472    Target accuracy: 0.662   source accuracy: 0.311\n",
            "Epoch0042/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2929       s_domain_loss: 0.5466   t_domain_loss:0.6937       grl_lambda: 0.481    Target accuracy: 0.644   source accuracy: 0.309\n",
            "Epoch0043/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3042       s_domain_loss: 0.6836   t_domain_loss:0.6470       grl_lambda: 0.491    Target accuracy: 0.669   source accuracy: 0.370\n",
            "Epoch0044/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3035       s_domain_loss: 0.6971   t_domain_loss:0.6622       grl_lambda: 0.500    Target accuracy: 0.614   source accuracy: 0.308\n",
            "Epoch0045/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3204       s_domain_loss: 0.6824   t_domain_loss:0.7482       grl_lambda: 0.510    Target accuracy: 0.636   source accuracy: 0.435\n",
            "Epoch0046/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3080       s_domain_loss: 0.6398   t_domain_loss:0.7475       grl_lambda: 0.519    Target accuracy: 0.630   source accuracy: 0.283\n",
            "Epoch0047/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3165       s_domain_loss: 0.7410   t_domain_loss:0.5998       grl_lambda: 0.528    Target accuracy: 0.630   source accuracy: 0.349\n",
            "Epoch0048/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2981       s_domain_loss: 0.6517   t_domain_loss:0.6682       grl_lambda: 0.537    Target accuracy: 0.639   source accuracy: 0.457\n",
            "Epoch0049/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2770       s_domain_loss: 0.6492   t_domain_loss:0.7117       grl_lambda: 0.546    Target accuracy: 0.640   source accuracy: 0.398\n",
            "Epoch0050/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2744       s_domain_loss: 0.7142   t_domain_loss:0.6395       grl_lambda: 0.554    Target accuracy: 0.619   source accuracy: 0.343\n",
            "Epoch0051/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2942       s_domain_loss: 0.6740   t_domain_loss:0.6930       grl_lambda: 0.563    Target accuracy: 0.659   source accuracy: 0.317\n",
            "Epoch0052/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2822       s_domain_loss: 0.7241   t_domain_loss:0.6504       grl_lambda: 0.571    Target accuracy: 0.633   source accuracy: 0.373\n",
            "Epoch0053/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2686       s_domain_loss: 0.6864   t_domain_loss:0.7142       grl_lambda: 0.580    Target accuracy: 0.604   source accuracy: 0.268\n",
            "Epoch0054/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2803       s_domain_loss: 0.6410   t_domain_loss:0.7095       grl_lambda: 0.588    Target accuracy: 0.613   source accuracy: 0.405\n",
            "Epoch0055/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2748       s_domain_loss: 0.7211   t_domain_loss:0.6516       grl_lambda: 0.596    Target accuracy: 0.612   source accuracy: 0.293\n",
            "Epoch0056/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2559       s_domain_loss: 0.6444   t_domain_loss:0.6857       grl_lambda: 0.604    Target accuracy: 0.651   source accuracy: 0.357\n",
            "Epoch0057/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2667       s_domain_loss: 0.6707   t_domain_loss:0.6903       grl_lambda: 0.612    Target accuracy: 0.655   source accuracy: 0.355\n",
            "Epoch0058/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2754       s_domain_loss: 0.6818   t_domain_loss:0.6300       grl_lambda: 0.620    Target accuracy: 0.625   source accuracy: 0.356\n",
            "Epoch0059/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2703       s_domain_loss: 0.6777   t_domain_loss:0.6779       grl_lambda: 0.627    Target accuracy: 0.619   source accuracy: 0.347\n",
            "Epoch0060/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2718       s_domain_loss: 0.6461   t_domain_loss:0.6326       grl_lambda: 0.635    Target accuracy: 0.656   source accuracy: 0.457\n",
            "Epoch0061/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2746       s_domain_loss: 0.6602   t_domain_loss:0.6847       grl_lambda: 0.642    Target accuracy: 0.654   source accuracy: 0.379\n",
            "Epoch0062/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2706       s_domain_loss: 0.6681   t_domain_loss:0.6077       grl_lambda: 0.650    Target accuracy: 0.601   source accuracy: 0.289\n",
            "Epoch0063/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.3037       s_domain_loss: 0.6564   t_domain_loss:0.7001       grl_lambda: 0.657    Target accuracy: 0.594   source accuracy: 0.378\n",
            "Epoch0064/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2910       s_domain_loss: 0.6714   t_domain_loss:0.7184       grl_lambda: 0.664    Target accuracy: 0.628   source accuracy: 0.307\n",
            "Epoch0065/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2927       s_domain_loss: 0.6699   t_domain_loss:0.6705       grl_lambda: 0.671    Target accuracy: 0.644   source accuracy: 0.336\n",
            "Epoch0066/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2689       s_domain_loss: 0.6961   t_domain_loss:0.6830       grl_lambda: 0.678    Target accuracy: 0.640   source accuracy: 0.303\n",
            "Epoch0067/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2788       s_domain_loss: 0.6859   t_domain_loss:0.6507       grl_lambda: 0.684    Target accuracy: 0.634   source accuracy: 0.426\n",
            "Epoch0068/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2872       s_domain_loss: 0.7149   t_domain_loss:0.6684       grl_lambda: 0.691    Target accuracy: 0.646   source accuracy: 0.322\n",
            "Epoch0069/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2749       s_domain_loss: 0.6933   t_domain_loss:0.7001       grl_lambda: 0.697    Target accuracy: 0.641   source accuracy: 0.393\n",
            "Epoch0070/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2972       s_domain_loss: 0.6431   t_domain_loss:0.6632       grl_lambda: 0.704    Target accuracy: 0.598   source accuracy: 0.331\n",
            "Epoch0071/ 0400\n",
            "=============\n",
            "[30/30]class_loss:  0.2969       s_domain_loss: 0.6059   t_domain_loss:0.7201       grl_lambda: 0.710    Target accuracy: 0.606   source accuracy: 0.309\n",
            "Epoch0072/ 0400\n",
            "=============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-6cd94bf243ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0maccuracy1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0ms_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0ms_logps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrl_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/svhn.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZf9-dEIFEv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}